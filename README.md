# Pasta Pipeline â€“ Telegram \& Telegraph ETL with Apache Airflow

> An end-to-end, container-first pipeline that scrapes messages from **@mrakopedia**, extracts Telegraph articles, cleans duplicates and stores everything in Supabase-hosted PostgreSQL via **Apache Airflow 2.7** [^1].

---

## âœ¨ Key Features

* **Multi-stage ETL** â€“ scrape Telegram, enrich with hashtags, crawl Telegraph pages, upsert into Postgres and run weekly duplicate cleanup in one DAG family [^1][^2].
* **Four processing modes** â€“ `incremental`, `refresh_old`, `daily`, `full` â€“ selectable through Airflow params for flexible back-filling and re-processing [^1].
* **Robust connection handling** â€“ SQLAlchemy pools with `pool_pre_ping`, `pool_recycle` and retry context managers keep long-lived Supabase SSL connections healthy [^1].
* **Batch + async fetching** â€“ Telegraph links processed in configurable batches with `aiohttp` + `asyncio` for fast throughput while respecting rate limits [^1].
* **Automatic duplicate remover** â€“ weekly DAG removes duplicate Telegram messages \& Telegraph rows and clears orphaned links for data hygiene [^1].
* **Docker-first deployment** â€“ single `docker-compose.yml` spins up web-server, scheduler and health-checks with 1 GB memory limits per service [^1].

---

## ğŸ“‚ Repository Layout

```
pasta-pipeline/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag.py               # DAG factory â€“ creates 4 DAG variants
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ message_scraper.py   # Telethon scraper
â”‚   â”‚   â”œâ”€â”€ db_loader.py         # Batched upserts into Postgres
â”‚   â”‚   â””â”€â”€ telegraph_processor.py# Async Telegraph crawler
â”‚   â”œâ”€â”€ utils/               # Regex helpers
â”‚   â””â”€â”€ duplicate_remover.py # Weekly cleanup tasks
â”œâ”€â”€ docker-compose.yml       # Two-service Airflow stack
â”œâ”€â”€ Dockerfile               # Extends apache/airflow:2.7.2
â””â”€â”€ README.md                # You are here
```


---

## ğŸ— Architecture \& Flow

```
Telegram âœ Message Scraper âœ telegram_messages
                 â”‚
                 â–¼
        Link Extractor (SQL)
                 â”‚
                 â–¼
        Telegraph Processor âœ telegraph_content
                 â”‚
                 â–¼
      Duplicate Cleanup DAG  (weekly)
```

1. **Scrape messages** with Telethon and save raw JSON/CSV artefacts [^2].
2. **Load into Postgres** with batched `INSERT â€¦ ON CONFLICT` upserts .
3. **Select links** according to DAG parameters (`incremental`, `refresh_old`, etc.) [^1].
4. **Async fetch + parse** Telegraph pages, compute MD5 hashes and store content .
5. **Deduplicate weekly** to keep only freshest message rows and unify identical content hashes .

---

## ğŸ›  Tech Stack

| Layer | Tool / Library | Why itâ€™s used |
| :-- | :-- | :-- |
| Orchestration | Apache Airflow 2.7 | DAG scheduling \& retries [^1] |
| Messaging API | Telethon 1.34 | Full Telegram MTProto access [^2] |
| HTML Parsing | BeautifulSoup 4 | Resilient content extraction |
| Async HTTP | aiohttp 3.9 | Non-blocking Telegraph pulls |
| Storage | PostgreSQL 15 (on Supabase) | ACID + JSONB support |
| Data Frames | pandas 2.x | CSV/JSON transform utilities |
| Containers | Docker \& Compose | Repeatable local dev \& prod |


---

## âš™ï¸ Quick Start

1. **Clone \& configure**

```bash
git clone https://github.com/your-org/pasta-pipeline.git
cd pasta-pipeline
cp .env.example .env  # fill Telegram & Supabase creds
```

2. **Build \& run**

```bash
docker compose up â€‘-build
```

3. **Open Airflow UI** â€“ http://localhost:8080, login `admin / $AIRFLOW_ADMIN_PASSWORD` [^1].
4. **Trigger DAGs**
    * `pasta_pipeline` â€“ main daily incremental run [^1].
    * `pasta_pipeline_full` â€“ manual full back-fill without limits [^1].
    * `pasta_duplicate_cleanup` â€“ weekly duplicate purge .

---

## ğŸ”‘ Environment Variables (.env)

| Name | Description |
| :-- | :-- |
| `API_ID`, `API_HASH`, `PHONE_NUMBER` | Telegram API credentials [^2] |
| `SESSION_STRING` | Saved Telethon session for headless login [^2] |
| `AIRFLOW_SQL_CONN` | Supabase Postgres URI (session mode 5432) |
| `AIRFLOW_SECRET_KEY` | Flask secret key for Airflow UI [^1] |
| `AIRFLOW_ADMIN_PASSWORD` | Initial Airflow admin password [^1] |


---

## ğŸ“ Database Schema (simplified)

### telegram_messages

| Column | Type | Purpose |
| :-- | :-- | :-- |
| message_id | BIGINT PK | Telegram ID |
| text / date / views / forwards | â€” | Raw metadata |
| hashtags | TEXT[] | Extracted hashtags |
| telegraph_link | TEXT | First Telegraph URL |
| reactions | JSONB | Emoji â†’ count |
| scraped_at / processed_at / last_updated | TIMESTAMP | Data lineage |

### telegraph_content

| Column | Type | Purpose |
| :-- | :-- | :-- |
| url PK | TEXT | Telegraph URL |
| title / content / description | â€” | Parsed article fields |
| content_hash / description_hash | TEXT | Change detection |
| processed_at / last_checked | TIMESTAMP | Versioning |
| word_count / status / retry_count | â€” | Monitoring helpers |


---

## ğŸ§¹ Duplicate Removal Strategy

* **telegram_messages** â€“ keep latest `processed_at`, drop older duplicates per `message_id` .
* **telegraph_content** â€“ keep first URL per identical `content_hash`, delete rest .
* **Orphan cleaner** â€“ nullifies stale `telegraph_link` values not present in content table .

Weekly DAG `pasta_duplicate_cleanup` executes these SQL scripts automatically [^1].

---

Hot-reload Airflow code changes by simply restarting the affected container:

```bash
docker compose restart airflow-webserver
```


---
