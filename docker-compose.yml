version: '3.8'

services:
  airflow-webserver:
    build: .
    container_name: pasta-airflow-webserver
    environment:
      # Airflow core settings
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_SQL_CONN}
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__DAG_FILE_PROCESSOR_TIMEOUT=600
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=30
      
      # Webserver settings
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__WEBSERVER__WORKERS=4
      - AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE=1
      - AIRFLOW__WEBSERVER__WORKER_REFRESH_INTERVAL=30
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=false
      
      # Scheduler settings
      - AIRFLOW__SCHEDULER__HEARTBEAT_INTERVAL=5
      - AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC=5
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=5
      
      # Logging
      - AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
      - AIRFLOW__LOGGING__FAB_LOGGING_LEVEL=WARN
      - AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS=
      
      # API settings
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
      
      # Telegram API credentials
      - API_ID=${API_ID}
      - API_HASH=${API_HASH}
      - PHONE_NUMBER=${PHONE_NUMBER}
      - SESSION_STRING=${SESSION_STRING}
      - CHANNEL_NAME=${CHANNEL_NAME:-@mrakopedia}
      
      # Database connection
      - AIRFLOW_CONN_POSTGRES_DEFAULT=${AIRFLOW_SQL_CONN}
      
      # Pipeline configuration
      - PROCESSING_MODE=${PROCESSING_MODE:-incremental}
      - TIME_LIMIT_DAYS=${TIME_LIMIT_DAYS:-7}
      - MAX_MESSAGES_PER_BATCH=${MAX_MESSAGES_PER_BATCH:-1000}
      - DB_BATCH_SIZE=${DB_BATCH_SIZE:-100}
      - TELEGRAPH_MAX_RETRIES=${TELEGRAPH_MAX_RETRIES:-3}
      - TELEGRAPH_TIMEOUT=${TELEGRAPH_TIMEOUT:-30}
      - TELEGRAPH_RATE_LIMIT=${TELEGRAPH_RATE_LIMIT:-1}
      - DATA_RETENTION_DAYS=${DATA_RETENTION_DAYS:-90}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./.env:/opt/airflow/.env
      - ./configs:/opt/airflow/configs
      - ./pasta-session.session:/opt/airflow/pasta-session.session
      - ./plugins:/opt/airflow/plugins
      - ./logs:/opt/airflow/logs
      
    ports:
      - "${AIRFLOW_WEBSERVER_PORT:-8080}:8080"
      
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
      
    restart: unless-stopped
    mem_limit: 1g
    cpus: 1.0
    
    command: >
      bash -c "
        airflow db migrate &&
        (airflow users get --username admin || airflow users create --username admin --password ${AIRFLOW_ADMIN_PASSWORD} --firstname Admin --lastname User --role Admin --email admin@example.com) &&
        airflow webserver
      "

  airflow-scheduler:
    build: .
    container_name: pasta-airflow-scheduler
    environment:
      # Airflow core settings
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW_SQL_CONN}
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__DAG_FILE_PROCESSOR_TIMEOUT=600
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=30
      
      # Webserver settings
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      
      # Scheduler settings
      - AIRFLOW__SCHEDULER__HEARTBEAT_INTERVAL=5
      - AIRFLOW__SCHEDULER__JOB_HEARTBEAT_SEC=5
      - AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=5
      - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=300
      
      # Logging
      - AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
      - AIRFLOW__LOGGING__FAB_LOGGING_LEVEL=WARN
      
      # Telegram API credentials
      - API_ID=${API_ID}
      - API_HASH=${API_HASH}
      - PHONE_NUMBER=${PHONE_NUMBER}
      - SESSION_STRING=${SESSION_STRING}
      - CHANNEL_NAME=${CHANNEL_NAME:-@mrakopedia}
      
      # Database connection
      - AIRFLOW_CONN_POSTGRES_DEFAULT=${AIRFLOW_SQL_CONN}
      
      # Pipeline configuration
      - PROCESSING_MODE=${PROCESSING_MODE:-incremental}
      - TIME_LIMIT_DAYS=${TIME_LIMIT_DAYS:-7}
      - MAX_MESSAGES_PER_BATCH=${MAX_MESSAGES_PER_BATCH:-1000}
      - DB_BATCH_SIZE=${DB_BATCH_SIZE:-100}
      - TELEGRAPH_MAX_RETRIES=${TELEGRAPH_MAX_RETRIES:-3}
      - TELEGRAPH_TIMEOUT=${TELEGRAPH_TIMEOUT:-30}
      - TELEGRAPH_RATE_LIMIT=${TELEGRAPH_RATE_LIMIT:-1}
      - DATA_RETENTION_DAYS=${DATA_RETENTION_DAYS:-90}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./configs:/opt/airflow/configs
      - ./data:/opt/airflow/data
      - ./.env:/opt/airflow/.env
      - ./pasta-session.session:/opt/airflow/pasta-session.session
      - ./logs:/opt/airflow/logs
      
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--hostname", "$(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
      
    restart: unless-stopped
    mem_limit: 1g
    cpus: 1.0
    
    command: airflow scheduler

  # Optional: Add monitoring with Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: pasta-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    profiles:
      - monitoring

  # Optional: Add Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: pasta-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped
    profiles:
      - monitoring

volumes:
  prometheus_data:
  grafana_data:

networks:
  default:
    name: pasta-network
